{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emo_detecttext(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI-e4KItRxri"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1_Jlj_dR4jA"
      },
      "source": [
        "#ls drive\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/datasets/emotion_nlp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag_mGcSER-jB"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, optimizers, losses, activations\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "\n",
        "from tensorflow.keras.callbacks import  EarlyStopping, ModelCheckpoint\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import os\n",
        "#import glob\n",
        "import random\n",
        "from google.colab import files #library to upload files to colab notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spVNhL3uUoA1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZQbKI5OWPCt"
      },
      "source": [
        "#load train data\n",
        "df_train = pd.read_csv('train.txt', sep =';', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsCuNZOuWhNs"
      },
      "source": [
        "#load test data\n",
        "df_test = pd.read_csv('test.txt', sep =';', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUEZ3oOcWptA"
      },
      "source": [
        "#rename columns\n",
        "df_test.columns = df_train.columns = ['text', 'emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0e_5lE5WtyS"
      },
      "source": [
        "#emotions\n",
        "df_train.describe()\n",
        "#we have 16000 sentences - 6 unique emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B8XP_cZW-SV"
      },
      "source": [
        "#emotions\n",
        "emotions = df_train.emotion.unique().tolist()\n",
        "emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jFLh4i_XgJd"
      },
      "source": [
        "#% of each emotion in the dataset\n",
        "np.round(df_train.emotion.value_counts()/len(df_train),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBIYDUa5XwWn"
      },
      "source": [
        "#some visualization\n",
        "sns.barplot(x = df_train.emotion.value_counts().index,   \n",
        "            y = df_train.emotion.value_counts())\n",
        "plt.title('emotions repartition')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXxuWbXTYOHf"
      },
      "source": [
        "df_train.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjTRwsU2bprk"
      },
      "source": [
        "emotions_dico = dict(zip(emotions,range(len(emotions))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd7KsPx8b9_l"
      },
      "source": [
        "emotions_dico"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqvAqoEKcAhx"
      },
      "source": [
        "df_train['label'] = df_train['emotion'].apply(lambda x: emotions_dico[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hA3J7hciQg"
      },
      "source": [
        "df_test['label'] = df_test['emotion'].apply(lambda x: emotions_dico[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjC4N570cqdU"
      },
      "source": [
        "train_sentences = df_train.text.tolist()\n",
        "y_train = df_train.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDZ0yUkZcsja"
      },
      "source": [
        "test_sentences = df_test.text.tolist()\n",
        "y_test = df_test.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3tjbEs6dCWo"
      },
      "source": [
        "MAX_VOCAB_SIZE = 50000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-xA3VxIe9wE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pJJnUTOfY_Y"
      },
      "source": [
        "#train_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnBmS25ueBNr"
      },
      "source": [
        "Max_padd = max([len(x) for x in train_sequences])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKuN78lneED_"
      },
      "source": [
        "train_sequences = pad_sequences(train_sequences,padding='post', maxlen=Max_padd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_sSY4aFgSxx"
      },
      "source": [
        "train_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGDtYTpxgjPl"
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(texts=test_sentences)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=Max_padd, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0Xt4zr2g8Rx"
      },
      "source": [
        "test_sequences.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xw6QIN5qUx1"
      },
      "source": [
        "#our vocab size\n",
        "len(tokenizer.index_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQtJqUBlqZ5P"
      },
      "source": [
        "T = train_sequences.shape[1]\n",
        "V =  len(tokenizer.index_word)\n",
        "K = len(emotions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sNSZyxgs2oS"
      },
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l9xgXIHuD3D"
      },
      "source": [
        "\n",
        "# using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
        "\n",
        "# save the best model with lower validation loss\n",
        "checkpointer = ModelCheckpoint(filepath = \"emodetext_weights.hdf5\", verbose = 1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7zFss0KsKxg"
      },
      "source": [
        "# Create the model\n",
        "\n",
        "# We get to choose embedding dimensionality\n",
        "D = 20\n",
        "\n",
        "# Hidden state dimensionality\n",
        "M = 15\n",
        "\n",
        "# Note: we actually want to the size of the embedding to (V + 1) x D,\n",
        "# because the first index starts from 1 and not 0.\n",
        "# Thus, if the final index of the embedding matrix is V,\n",
        "# then it actually must have size V + 1.\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = LSTM(M, return_sequences=True)(x)\n",
        "#x = LSTM(M, return_sequences=True)(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(K, activation='softmax')(x)\n",
        "\n",
        "model = Model(i, x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1OGAvvsgCF"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nIE8PN0t4tg"
      },
      "source": [
        "history = model.fit(x=train_sequences, y = y_train, epochs=20, validation_data=(test_sequences,y_test),\n",
        "                    callbacks=[checkpointer, earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPlyvkf7uTkm"
      },
      "source": [
        "model.save('emodetext_model_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZLCndMuAUUX"
      },
      "source": [
        "model = load_model('emodetext_model_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_GEZboN0Wk-"
      },
      "source": [
        "y_pred = model.predict(test_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRY3IDJ0cB4"
      },
      "source": [
        "y_pred = np.argmax(y_pred,axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EeKzeh_0dAn"
      },
      "source": [
        "y_test = np.argmax(y_test,\n",
        "                   axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9jM6cYs2Y5t"
      },
      "source": [
        "emotions_dico = dict(zip(emotions_dico.values(), emotions_dico.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXzWnBjg0xEY"
      },
      "source": [
        "import random\n",
        "#let see some prediction\n",
        "for i in range(10):\n",
        "  j = random.randint(0,len(y_test))\n",
        "  print('Text:', test_sentences[j])\n",
        "  print('prediction: ', emotions_dico[y_pred[j]])\n",
        "  print('real emotion : ',emotions_dico[y_test[j]])\n",
        "  print('**********************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldSccqHQ1BL5"
      },
      "source": [
        "#some predictions\n",
        "my_text = ['i am felling very good', 'i am so loved']\n",
        "my_seqence = tokenizer.texts_to_sequences(my_text)\n",
        "my_seqence = pad_sequences(my_seqence, maxlen=Max_padd, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp4seMfV3xqo"
      },
      "source": [
        "my_pred = np.argmax( model.predict(my_seqence), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbmkREbQ4bw0"
      },
      "source": [
        "[emotions_dico[x] for x in my_pred ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC5oMFuG5HL5"
      },
      "source": [
      ]
    }
  ]
}
